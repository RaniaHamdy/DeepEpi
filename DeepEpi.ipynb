{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLx-RgPK-JPk"
      },
      "outputs": [],
      "source": [
        "#for using GPU and high RAM from Colab \n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else: \n",
        "  print('You are using a high-RAM runtime!')\n",
        "\n",
        "#get acess to goolgle drive \n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')## data set in goole drive "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#self attention keras liberary\n",
        "!pip install keras-self-attention"
      ],
      "metadata": {
        "id": "z0qCqhlh-vVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras_preprocessing import image\n",
        "from keras import preprocessing\n",
        "from keras import datasets\n",
        "# Basic packages  \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "import statistics\n",
        "# Packages for data preparation\n",
        "from keras.layers.core import Reshape\n",
        "from sklearn import preprocessing\n",
        "from keras_preprocessing import sequence\n",
        "# Package For Imbalanced Data Distribution\n",
        "from imblearn.over_sampling import SMOTE \n",
        "# Packages for modeling  \n",
        "import keras \n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "import cProfile\n",
        "from typing import List, Tuple\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import keras.layers\n",
        "from keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Activation \n",
        "from tensorflow.keras.layers import Conv1D,Convolution1D, SeparableConv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D,AveragePooling1D,GlobalAveragePooling1D,GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.layers import Conv2D,Convolution2D,ConvLSTM2D,Conv2DTranspose, DepthwiseConv2D\n",
        "from tensorflow.keras.layers import AveragePooling2D,MaxPooling2D, GlobalMaxPooling2D,ZeroPadding2D,MaxPooling3D\n",
        "from tensorflow.keras.layers import Bidirectional,Activation,Flatten,Dense ,BatchNormalization\n",
        "from keras.layers import LSTM ,RepeatVector\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM   # CuDNNLSTM not yet released for TF 2.0\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.python.keras import Input\n",
        "from keras.utils import layer_utils\n",
        "tf.compat.v1.keras.layers.Attention\n",
        "tf.compat.v1.keras.layers.Bidirectional\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.python.keras.layers import Concatenate\n",
        "\n",
        "# Packages for Data overfitting\n",
        "from keras.layers import Dropout ,SpatialDropout1D\n",
        "from keras import regularizers\n",
        "# Packages for Data evaluation and tunning\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import cross_val_score ,cross_val_predict ,cross_val_predict\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#Model tuning \n",
        "#Layer weight initializersinitializers *\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.initializers import he_uniform\n",
        "from tensorflow.keras.initializers import glorot_normal\n",
        "\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# Packages for Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "\n",
        "#from keras_applications.resnet import ResNet50\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "\n",
        "\n",
        "# problem in LStm graph(just for colap)\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "K.clear_session()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "#tf.executing_eagerly()\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "print(tf.executing_eagerly())\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras import initializers\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "#from keras_multi_head import MultiHead\n",
        "\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.models import Model, load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.base import clone\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from multiprocessing import Queue\n",
        "print(\"done\")\n",
        "checkpointer = ModelCheckpoint(filepath=\"/content/gdrive/My Drive/modelgraph/model_seqHMDilated.h5\",\n",
        "                               verbose=0,\n",
        "                              monitor='val_loss', save_weights_only=True, save_best_only=True)\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "from sklearn.metrics import classification_report\n",
        "disable_eager_execution()\n",
        "tf.function\n",
        "#tf.compat.v1.function\n",
        "seed =123\n",
        "np.random.seed(seed)\n",
        "peephole_lstm_cells=[]"
      ],
      "metadata": {
        "id": "Fm0i8ud_-vZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_data():\n",
        "    all_X_train_files=list(filter(lambda x: '_rc_x_train.csv' in x, os.listdir('/content/gdrive/My Drive/HM Dataset')))\n",
        "    all_X_train_files.sort() \n",
        "    all_y_train_files=list(filter(lambda x: '_rc_y_train.csv' in x, os.listdir('/content/gdrive/My Drive/HM Dataset')))\n",
        "    all_y_train_files.sort() \n",
        "    all_X_test_files=list(filter(lambda x: '_rc_x_test.csv' in x, os.listdir('/content/gdrive/My Drive/HM Dataset')))\n",
        "    all_X_test_files.sort()\n",
        "    all_y_test_files=list(filter(lambda x: '_rc_y_test.csv' in x, os.listdir('/content/gdrive/My Drive/HM Dataset')))\n",
        "    all_y_test_files.sort() \n",
        "    return all_X_train_files,all_y_train_files,all_X_test_files,all_y_test_files\n",
        "\n",
        "\n",
        "def load_data(fileindex):\n",
        "    all_X_train_files,all_y_train_files,all_X_test_files,all_y_test_files= list_data()\n",
        "    X_train_file = all_X_train_files[fileindex]\n",
        "    y_train_file = all_y_train_files[fileindex]\n",
        "    X_test_file = all_X_test_files[fileindex]\n",
        "    y_test_file = all_y_test_files[fileindex]\n",
        "    file_name = X_train_file\n",
        "    x_train = np.loadtxt('/content/gdrive/My Drive/HM Dataset' + os.sep + X_train_file, delimiter = \",\")\n",
        "    x_test  = np.loadtxt('/content/gdrive/My Drive/HM Dataset'+ os.sep + X_test_file , delimiter = \",\") \n",
        "    y_train = np.loadtxt('/content/gdrive/My Drive/HM Dataset' + os.sep + y_train_file, delimiter = \",\")\n",
        "    y_test  = np.loadtxt('/content/gdrive/My Drive/HM Dataset' + os.sep + y_test_file , delimiter = \",\") \n",
        "    return x_train,x_test,y_train,y_test,file_name\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n",
        "\n",
        "input_shape = (100,5)\n",
        "checkpointer = ModelCheckpoint(filepath=\"model_seqHMDilated.h5\",\n",
        "                               verbose=0,\n",
        "                              monitor='val_loss', save_weights_only=True, save_best_only=True)\n",
        "print(\"Done\")\n",
        "\n",
        "def SMOTE(X_train,y_train):\n",
        " \n",
        "  nsamples, nx, ny = X_train.shape\n",
        "  X_train2d = X_train.reshape((nsamples,nx*ny))\n",
        "  #print(X_train2d)\n",
        "  from imblearn.over_sampling import SMOTE \n",
        "  sm = SMOTE(random_state = 2) \n",
        "  X_train_res, y_train_res = sm.fit_sample(X_train2d, y_train.ravel()) \n",
        "  nsamples1, nx1 = X_train_res.shape\n",
        "  X_train=X_train_res.reshape((nsamples1,nx,ny))\n",
        "  return X_train,y_train_res"
      ],
      "metadata": {
        "id": "Agc13Vw0-vcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using  data set for LSTM \n",
        "def split_data(fileindex): \n",
        "    x_train,x_test,y_train,y_test,file_name=load_data(fileindex)\n",
        "    \n",
        "    y_train= y_train[:,1] \n",
        "    y_test =  y_test[:,1]\n",
        "    #split X_train 5 column(5 Histone)\n",
        "    X_train = x_train[:,1:6] \n",
        "    X_test  = x_test[:,1:6]   \n",
        "\n",
        "    #count gene numbers each gene 100 bin\n",
        "    num_genes_train = X_train.shape[0] / 100\n",
        "    num_genes_test  = X_test.shape[0] / 100\n",
        "    #\"np.split\"This function divides the array into subarrays along a specified axis\n",
        "    X_train = np.split(X_train, num_genes_train)\n",
        "    X_test  = np.split(X_test, num_genes_test)\n",
        "\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test  = np.array(X_test)\n",
        "    # convert data from list to array\n",
        "    y_train = np.array(y_train)\n",
        "    y_test  = np.array(y_test)\n",
        "\n",
        "    file_name=file_name\n",
        "    print(file_name)  \n",
        "    return y_train,y_test,X_train,X_test,file_name\n",
        "    \n",
        "def get_single_histone(X_train,X_test):\n",
        "  Single_Histone_x_train = np.array_split(X_train, 5, axis=2)\n",
        "  #print(Single_Histone_x_train)\n",
        "  h1_x_train= Single_Histone_x_train[0]\n",
        "  h2_x_train= Single_Histone_x_train[1]\n",
        "  h3_x_train= Single_Histone_x_train[2]\n",
        "  h4_x_train= Single_Histone_x_train[3]\n",
        "  h5_x_train= Single_Histone_x_train[4]\n",
        "  #print(h1_x_train)\n",
        "#__________________________________________________\n",
        "  Single_Histone_x_test = np.array_split(X_test, 5, axis=2)\n",
        "  #print(Single_Histone_x_test)\n",
        "  h1_x_test= Single_Histone_x_test[0]\n",
        "  h2_x_test= Single_Histone_x_test[1]\n",
        "  h3_x_test= Single_Histone_x_test[2]\n",
        "  h4_x_test= Single_Histone_x_test[3]\n",
        "  h5_x_test= Single_Histone_x_test[4]\n",
        "  #print(h1_x_test)\n",
        "  return h1_x_train, h2_x_train, h3_x_train, h4_x_train, h5_x_train, h1_x_test, h2_x_test, h3_x_test, h4_x_test, h5_x_test "
      ],
      "metadata": {
        "id": "foMphef--vfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-LSTM\n",
        "seed =123\n",
        "#peephole_lstm_cells = [PeepholeLSTMCell(32) for size in [32, 64]]\n",
        "np.random.seed(seed)\n",
        "def attention_LSTM():\n",
        "  sequence_input = Input(shape=(100,5))\n",
        "  #sequence_input = Input(shape=(X_train.shape[0],X_train.shape[1],X_train.shape[2]))\n",
        "  #x= Reshape((100,50))(x)\n",
        "  x = CuDNNLSTM(32, return_sequences=True, name='Lstm1' )(sequence_input)\n",
        "  x = CuDNNLSTM(16, return_sequences=True )(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x = CuDNNLSTM(8, return_sequences=True )(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128, activation=\"relu\")(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  preds = Dense(1,activation='sigmoid')(x)\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate= 0.005)\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='binary_crossentropy',optimizer= optimizer ,metrics=['accuracy',keras.metrics.AUC()])\n",
        "  #model.compile(loss='binary_crossentropy',optimizer= \"Adam\",metrics=['accuracy',keras.metrics.AUC()])\n",
        "  #model.summary()\n",
        "  #plot_model(model, to_file='model/cnn_lstm.png')\n",
        "  model.fit(x_tr, y_tr, batch_size=300,epochs=10, validation_data=(x_va, y_va), verbose=1,callbacks = [checkpointer])\n",
        "  return model\n",
        "\n",
        "def get_auc():\n",
        "  model= attention_LSTM()\n",
        "  predictions_NN_prob = model.predict(X_test)\n",
        "  auc = roc_auc_score(y_test, predictions_NN_prob)\n",
        "  print('AUC: %.3f' % auc)\n",
        "  list_of_numbers.append(auc)\n",
        "  print(list_of_numbers)\n",
        "  return list_of_numbers\n",
        "\n",
        "list_of_numbers=[]\n",
        "for i in range(0, 56):\n",
        "    y_train,y_test,X_train,X_test,file_name= split_data(i)\n",
        "    #X_train,y_train=SMOTE(X_train,y_train)\n",
        "    #split train set into train-val\n",
        "    x_tr, x_va, y_tr, y_va = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
        "    list_of_numbers = get_auc()\n",
        "    avg = sum(list_of_numbers)/len(list_of_numbers)\n",
        "    print(\"The average is \", round(avg,5))\n",
        "    i+1\n",
        "    \n",
        "[0.8860359109630245, 0.8702311314112936, 0.8894439896281124, 0.874036874784818, 0.8884788394371486, 0.8808553952380058, 0.894522379145801, 0.9008512290872961, 0.8951923106640087, 0.8923860929710683, 0.8775584055712033, 0.8782798820643439, 0.9007371111292655, 0.9084848794442175, 0.9096801632817989, 0.9105152291808374, 0.9074298990015505, 0.9073991564189389, 0.9109875023962501, 0.9035013161463037, 0.8914498954922465, 0.8939930337818119, 0.9021052039580431, 0.9087518545231681, 0.8984615599855198, 0.8361737569282225, 0.8802850095799486, 0.8857962213225371, 0.8508506768204792, 0.8622361542860494, 0.8922507612178061, 0.8774038064664068, 0.8790595481876993, 0.862052674215112, 0.852407824322738, 0.8908712004745621, 0.852146192939186, 0.8849068335020221, 0.8690721401776248, 0.8995421951466712, 0.8776303061208721, 0.8854055599338617, 0.8745409372635107, 0.8629477099712968, 0.8468088631837772, 0.8722432877609895, 0.9019289064596009, 0.9087668281467707, 0.9172386084040912, 0.913255212204126, 0.8951340663282026, 0.89871461153255, 0.9047638014694663, 0.9135831957309408, 0.8931293408280855, 0.8928764510748448]\n",
        "The average is  0.88777\n",
        "\n"
      ],
      "metadata": {
        "id": "MbGQwCEGFGGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2-BidirectionalLSTM\n",
        "def attention_LSTM():\n",
        "  #x= Sequential()\n",
        "  sequence_input = Input(shape=(100,5))\n",
        "  #sequence_input = Input(shape=(X_train.shape[0],X_train.shape[1],X_train.shape[2]))\n",
        "  #x= Reshape((100,50))(x)\n",
        "  x = Bidirectional(CuDNNLSTM(64, return_sequences=True),name='BILstm1')(sequence_input)\n",
        "  x = Bidirectional(CuDNNLSTM(32, return_sequences=True ),name='BILstm2')(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x = Bidirectional(CuDNNLSTM(16, return_sequences=True ),name='BILstm3')(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128, activation=\"relu\")(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  preds = Dense(1,activation='sigmoid',name='last')(x)\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate= 0.001)\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='binary_crossentropy',optimizer= optimizer ,metrics=['accuracy',keras.metrics.AUC()])\n",
        "  #model.compile(loss='binary_crossentropy',optimizer= \"Adam\",metrics=['accuracy',keras.metrics.AUC()])\n",
        "  model.summary()\n",
        "  plot_model(model, to_file='/content/gdrive/My Drive/modelgraph/BILSTM.png', show_shapes=True, show_layer_names=True, rankdir=\"TB\", dpi= 300, expand_nested=True,layer_range=['BILstm1','last'])\n",
        "  model.fit(x_tr, y_tr, batch_size=300,epochs=10, validation_data=(x_va, y_va), verbose=1,callbacks = [checkpointer])\n",
        "  return model\n",
        "\n",
        "def get_auc():\n",
        "  model= attention_LSTM()\n",
        "  predictions_NN_prob = model.predict(X_test)\n",
        "  auc = roc_auc_score(y_test, predictions_NN_prob)\n",
        "  print('AUC: %.3f' % auc)\n",
        "  list_of_numbers.append(auc)\n",
        "  print(list_of_numbers)\n",
        "  return list_of_numbers\n",
        "\n",
        "list_of_numbers=[]\n",
        "for i in range(0, 56):\n",
        "    y_train,y_test,X_train,X_test,file_name= split_data(i)\n",
        "    #X_train,y_train=SMOTE(X_train,y_train)\n",
        "    #split train set into train-val\n",
        "    x_tr, x_va, y_tr, y_va = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
        "    list_of_numbers = get_auc()\n",
        "    avg = sum(list_of_numbers)/len(list_of_numbers)\n",
        "    print(\"The average is \", round(avg,5))\n",
        "    i+1\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "woO2YR3UF_M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#3- LSTM+att\n",
        "seed =123\n",
        "np.random.seed(seed)\n",
        "def attention_LSTM():\n",
        "  #x= Sequential()\n",
        "  sequence_input = Input(shape=(100,5))\n",
        "  #sequence_input = Input(shape=(X_train.shape[0],X_train.shape[1],X_train.shape[2]))\n",
        "  #x= Reshape((100,50))(x)\n",
        "  x = CuDNNLSTM(64, return_sequences=True, name='Lstm1' )(sequence_input)\n",
        "  x=  SeqSelfAttention(attention_width=15,\n",
        "                      attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation=None)(x)\n",
        "  x = CuDNNLSTM(32, return_sequences=True )(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x=  SeqSelfAttention(attention_width=10,\n",
        "                       attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation=None)(x)\n",
        "  x = CuDNNLSTM(16, return_sequences=True )(x)\n",
        "  x=  SeqSelfAttention(attention_width=5,\n",
        "                       attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation=None)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128, activation=\"relu\")(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  preds = Dense(1,activation='sigmoid',name='last')(x)\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate= 0.001)\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='binary_crossentropy',optimizer= optimizer ,metrics=['accuracy',keras.metrics.AUC()])\n",
        "  #model.compile(loss='binary_crossentropy',optimizer= \"Adam\",metrics=['accuracy',keras.metrics.AUC()])\n",
        "  model.summary()\n",
        "  #visualizer(model, format='png', view=True)\n",
        "  #model.save('/content/gdrive/My Drive/modelgraph/attention_twolayerLSTM.h5')\n",
        "  plot_model(model, to_file='/content/gdrive/My Drive/modelgraph/LSTMatt.png', show_shapes=True, show_layer_names=True, rankdir=\"TB\", dpi= 300, expand_nested=True,layer_range=[\"Lstm1\",\"last\"])\n",
        "  model.fit(x_tr, y_tr, batch_size=300,epochs=10, validation_data=(x_va, y_va), verbose=1,callbacks = [checkpointer])\n",
        "  return model\n",
        "\n",
        "def get_auc():\n",
        "  model= attention_LSTM()\n",
        "  predictions_NN_prob = model.predict(X_test)\n",
        "  auc = roc_auc_score(y_test, predictions_NN_prob)\n",
        "  print('AUC: %.3f' % auc)\n",
        "  list_of_numbers.append(auc)\n",
        "  print(list_of_numbers)\n",
        "  return list_of_numbers\n",
        "\n",
        "list_of_numbers=[]\n",
        "for i in range(0, 56):\n",
        "    y_train,y_test,X_train,X_test,file_name= split_data(i)\n",
        "    #X_train,y_train=SMOTE(X_train,y_train)\n",
        "    #split train set into train-val\n",
        "    x_tr, x_va, y_tr, y_va = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
        "    list_of_numbers = get_auc()\n",
        "    avg = sum(list_of_numbers)/len(list_of_numbers)\n",
        "    print(\"The average is \", round(avg,5))\n",
        "    i+1\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T8ky704VGcHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4- 1DCNN and LSTM\n",
        "\n",
        "def split_data(fileindex): \n",
        "    x_train,x_test,y_train,y_test,file_name=load_data(fileindex)\n",
        "    \n",
        "    y_train= y_train[:,1] \n",
        "    y_test =  y_test[:,1]\n",
        "    #split X_train 5 column(5 Histone)\n",
        "    X_train = x_train[:,1:6] \n",
        "    X_test  = x_test[:,1:6]   \n",
        "\n",
        "    #count gene numbers each gene 100 bin\n",
        "    num_genes_train = X_train.shape[0] / 100\n",
        "    num_genes_test  = X_test.shape[0] / 100\n",
        "    #\"np.split\"This function divides the array into subarrays along a specified axis\n",
        "    X_train = np.split(X_train, num_genes_train)\n",
        "    X_test  = np.split(X_test, num_genes_test)\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test  = np.array(X_test)\n",
        "    # convert data from list to array\n",
        "    y_train = np.array(y_train)\n",
        "    y_test  = np.array(y_test)\n",
        "\n",
        "   \n",
        "    file_name=file_name\n",
        "    print(file_name)\n",
        "    \n",
        "    return y_train,y_test,X_train,X_test,file_name\n",
        "\n",
        "def CNN1D_LSTMselfatt():\n",
        "  #x= Sequential()\n",
        "  #sequence_input = Input(shape=(100,5))\n",
        "  sequence_input = Input(shape=(100,5))\n",
        "\n",
        "  x= Conv1D(50, 5, padding='same', activation= 'relu')(sequence_input)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x= Conv1D(40, 3, padding='same',  activation= 'relu')(x) \n",
        "  x = Dropout(0.4)(x)\n",
        "  x= Conv1D(30, 2, padding='same',  activation= 'relu')(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x= Reshape((100,30))(x)\n",
        "  x = CuDNNLSTM(32, return_sequences=True)(x)\n",
        "  x = CuDNNLSTM(16, return_sequences=True)(x)\n",
        "  x = CuDNNLSTM(8, return_sequences=True)(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128, activation=\"relu\")(x)\n",
        "  x = Dense(64, activation=\"relu\")(x)\n",
        "  #optimizer=tf.keras.optimizers.Adam(learning_rate= 0.005)\n",
        "\n",
        "  preds = Dense(1,activation='sigmoid')(x)\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy',keras.metrics.AUC()])\n",
        "  #model.summary()\n",
        "  #plot_model(model, to_file='model/cnn_lstm.png')\n",
        "  model.fit(x_tr, y_tr, batch_size=300,epochs=10, validation_data=(x_va, y_va), verbose=1,callbacks = [checkpointer])\n",
        "  return model\n",
        "\n",
        "\n",
        "def get_auc():\n",
        "  model= CNN1D_LSTMselfatt()\n",
        "  predictions_NN_prob = model.predict(X_test)\n",
        "  auc = roc_auc_score(y_test, predictions_NN_prob)\n",
        "  print('AUC: %.3f' % auc)\n",
        "  list_of_numbers.append(auc)\n",
        "  print(list_of_numbers)\n",
        "  return list_of_numbers\n",
        "\n",
        "list_of_numbers=[]\n",
        "for i in range(0, 56):\n",
        "    y_train,y_test,X_train,X_test,file_name= split_data(i)\n",
        "    #X_train,y_train=SMOTE(X_train,y_train)\n",
        "    #split train set into train-val\n",
        "    x_tr, x_va, y_tr, y_va = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
        "    list_of_numbers = get_auc()\n",
        "    avg = sum(list_of_numbers)/len(list_of_numbers)\n",
        "    print(\"The average is \", round(avg,5))\n",
        "    i+1"
      ],
      "metadata": {
        "id": "Wp16cDYhKQPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5- 1DCNN and LSTM +att\n",
        "\n",
        "def split_data(fileindex): \n",
        "    x_train,x_test,y_train,y_test,file_name=load_data(fileindex)\n",
        "    \n",
        "    y_train= y_train[:,1] \n",
        "    y_test =  y_test[:,1]\n",
        "    #split X_train 5 column(5 Histone)\n",
        "    X_train = x_train[:,1:6] \n",
        "    X_test  = x_test[:,1:6]   \n",
        "\n",
        "    #count gene numbers each gene 100 bin\n",
        "    num_genes_train = X_train.shape[0] / 100\n",
        "    num_genes_test  = X_test.shape[0] / 100\n",
        "    #\"np.split\"This function divides the array into subarrays along a specified axis\n",
        "    X_train = np.split(X_train, num_genes_train)\n",
        "    X_test  = np.split(X_test, num_genes_test)\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test  = np.array(X_test)\n",
        "    # convert data from list to array\n",
        "    y_train = np.array(y_train)\n",
        "    y_test  = np.array(y_test)\n",
        "\n",
        "   \n",
        "    file_name=file_name\n",
        "    print(file_name)\n",
        "    \n",
        "    return y_train,y_test,X_train,X_test,file_name\n",
        "\n",
        "def CNN1D_LSTMselfatt():\n",
        "  #x= Sequential()\n",
        "  #sequence_input = Input(shape=(100,5))\n",
        "  sequence_input = Input(shape=(100,5))\n",
        "\n",
        "  x= Conv1D(50, 5, padding='same', activation= 'relu')(sequence_input)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x= Conv1D(40, 3, padding='same',  activation= 'relu')(x) \n",
        "  x = Dropout(0.4)(x)\n",
        "  x= Conv1D(30, 2, padding='same',  activation= 'relu')(x)\n",
        "  x= Conv1D(20, 2, padding='same',  activation= 'relu')(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x= Reshape((100,30))(x)\n",
        "\n",
        "  x = CuDNNLSTM(32, return_sequences=True)(x)\n",
        "  x=  SeqSelfAttention(attention_width=15,\n",
        "                       attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation=None)(x)\n",
        "  x = CuDNNLSTM(16, return_sequences=True)(x)\n",
        "  x=  SeqSelfAttention(attention_width=5,\n",
        "                       attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation=None)(x)\n",
        "  x = CuDNNLSTM(8, return_sequences=True)(x)\n",
        "  x=  SeqSelfAttention(attention_width=2,\n",
        "                       attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation=None)(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128, activation=\"relu\")(x)\n",
        "  x = Dense(64, activation=\"relu\")(x)\n",
        "  #optimizer=tf.keras.optimizers.Adam(learning_rate= 0.005)\n",
        "\n",
        "  preds = Dense(1,activation='sigmoid')(x)\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy',keras.metrics.AUC()])\n",
        "  #model.summary()\n",
        "  #plot_model(model, to_file='model/cnn_lstm.png')\n",
        "  model.fit(x_tr, y_tr, batch_size=300,epochs=10, validation_data=(x_va, y_va), verbose=1,callbacks = [checkpointer])\n",
        "  return model\n",
        "\n",
        "list_of_Precision=[]\n",
        "list_of_Recall=[]\n",
        "list_of_f1=[]\n",
        "def get_auc():\n",
        "  model= CNN1D_LSTMselfatt()\n",
        "  predictions_NN_prob = model.predict(X_test)\n",
        "  auc = roc_auc_score(y_test, predictions_NN_prob)\n",
        "  print('AUC: %.3f' % auc)\n",
        "  y_pred = np.argmax(predictions_NN_prob, axis=1)\n",
        "  precision = precision_score(y_test, y_pred , average=\"macro\")\n",
        "  recall = recall_score(y_test, y_pred , average=\"macro\")\n",
        "  f1 = f1_score(y_test, y_pred , average=\"macro\")\n",
        "# Print f1, precision, and recall scores\n",
        "  print('precision_score: %.3f' % precision)\n",
        "  print('recall: %.3f' % recall)\n",
        "  print('f1_score: %.3f' % f1)\n",
        "  list_of_numbers.append(auc)\n",
        "  list_of_Precision.append(precision)\n",
        "  list_of_Recall.append(recall)\n",
        "  list_of_f1.append(f1)\n",
        "  print('AUC_list:',  list_of_numbers)\n",
        "  print('precision_list:', list_of_Precision)\n",
        "  print('Recall_list:',  list_of_Recall)\n",
        "  print('f1_list:',  list_of_f1)\n",
        "  return list_of_numbers, list_of_Precision, list_of_Recall, list_of_f1\n",
        "\n",
        "list_of_numbers=[]\n",
        "for i in range(0, 56):\n",
        "    y_train,y_test,X_train,X_test,file_name= split_data(i)\n",
        "    #print(xtrain.shape())\n",
        "    #X_train,y_train=SMOTE(X_train,y_train)\n",
        "    #split train set into train-val\n",
        "    x_tr, x_va, y_tr, y_va = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
        "    list_of_numbers, list_of_Precision, list_of_Recall, list_of_f1 = get_auc()\n",
        "    avg = sum(list_of_numbers)/len(list_of_numbers)\n",
        "    print(\"The AUC average is \", round(avg,5))\n",
        "    avg_Precision = sum(list_of_Precision)/len(list_of_Precision)\n",
        "    print(\"The Precision average is \", round(avg_Precision,5))\n",
        "    avg_Recall = sum(list_of_Recall)/len(list_of_Recall)\n",
        "    print(\"The Recall average is \", round(avg_Recall,5))\n",
        "    avg_f1  = sum(list_of_f1)/len(list_of_f1)\n",
        "    print(\"The f1 average is \", round(avg_f1 ,5))\n",
        "    i+1"
      ],
      "metadata": {
        "id": "sKqPs4Qe-viQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6- convlstm\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "def split_data(fileindex): \n",
        "    x_train,x_test,y_train,y_test,file_name=load_data(fileindex)\n",
        "    \n",
        "    y_train= y_train[:,1] \n",
        "    y_test =  y_test[:,1]\n",
        "    #split X_train 5 column(5 Histone)\n",
        "    X_train = x_train[:,1:6] \n",
        "    X_test  = x_test[:,1:6]   \n",
        "\n",
        "    #count gene numbers each gene 100 bin\n",
        "    num_genes_train = X_train.shape[0] / 100\n",
        "    num_genes_test  = X_test.shape[0] / 100\n",
        "    #\"np.split\"This function divides the array into subarrays along a specified axis\n",
        "    X_train = np.split(X_train, num_genes_train)\n",
        "    X_test  = np.split(X_test, num_genes_test)\n",
        "    X_train = np.array(X_train)\n",
        "    X_test  = np.array(X_test)\n",
        "    # convert data from list to array\n",
        "    y_train = np.array(y_train)\n",
        "    y_test  = np.array(y_test)\n",
        "\n",
        "    #X_train = np.reshape(X_train,(X_train.shape[0],1,X_train.shape[1],X_train.shape[2]))\n",
        "    #X_test = np.reshape(X_test,(X_test.shape[0],1, X_test.shape[1],X_test.shape[2]))\n",
        "\n",
        "\n",
        "    file_name=file_name\n",
        "    print(file_name)\n",
        "\n",
        "    return y_train,y_test,X_train,X_test,file_name\n",
        "\n",
        "seed =123\n",
        "np.random.seed(seed)\n",
        "def convLstm2d():\n",
        "  #sequence_input = Input(shape=(1,100,5))\n",
        "  #sequence_input = Input(shape=(1,x_tr.shape[0],x_tr.shape[1],x_tr.shape[2]))\n",
        "  sequence_input = Input(shape=(1, 100, 5,1))\n",
        "  x= ConvLSTM2D(50, (3,3), padding='same', activation= 'relu', return_sequences=True,dropout=0.4, recurrent_dropout=0.4, name ='ConvLSTM')(sequence_input)# dropout=0.4 \n",
        "  print(\"x from ConvLSTM2D =\", x.shape)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128, activation=\"relu\")(x)\n",
        "  #x = Dense(64, activation=\"relu\")(x)\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate= 0.001)\n",
        "  preds = Dense(1,activation='sigmoid',name='last')(x)\n",
        "  #print(preds.shape)\n",
        "  #print(sequence_input.shape)\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='binary_crossentropy',optimizer= optimizer,metrics=['accuracy',keras.metrics.AUC()])\n",
        "  #model.summary()\n",
        "  plot_model(model, to_file='/content/gdrive/My Drive/modelgraph/convlstm.png', show_shapes=True, show_layer_names=True, rankdir=\"TB\", dpi= 300, expand_nested=True,layer_range=[\"ConvLSTM\",\"last\"])\n",
        "  #print(x_tr.shape)\n",
        "  #print(y_tr.shape)\n",
        "  model.fit(x_tr, y_tr, batch_size=300,epochs=10, validation_data=(x_va, y_va), verbose=1,callbacks = [checkpointer])\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "def get_auc():\n",
        "  model= convLstm2d()\n",
        "  predictions_NN_prob = model.predict(X_test)\n",
        "\n",
        "  auc = roc_auc_score(y_test, predictions_NN_prob)\n",
        "  print('AUC: %.3f' % auc)\n",
        "  list_of_numbers.append(auc)\n",
        "  print(\"list_of_AUC\",list_of_numbers)\n",
        "    #predictions_NN = model.predict_proba(X_test)[:,1]\n",
        "    #auc_proba = roc_auc_score(y_test, predictions_NN)\n",
        "    #print('AUC proba: %.3f' % auc_proba)\n",
        "    #list_of_numbers_broba.append(auc_proba)\n",
        "    #print(\"list_of_AUC_proba\",list_of_numbers_broba)\n",
        "  return list_of_numbers #,list_of_numbers_broba\n",
        "\n",
        "list_of_numbers=[]\n",
        "#list_of_numbers_broba=[]\n",
        "for i in range(0, 56):\n",
        "    y_train,y_test,X_train,X_test,file_name= split_data(i)\n",
        "    #X_train,y_train=SMOTE(X_train,y_train)\n",
        "    #split train set into train-val\n",
        "    x_tr, x_va, y_tr, y_va = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
        "    x_tr = np.reshape(x_tr,(x_tr.shape[0],1, 100, 5,1))\n",
        "    x_va = np.reshape(x_va,(x_va.shape[0],1,100, 5,1))\n",
        "\n",
        "    #x_tr = np.reshape(x_tr,(12388,1, 100, 5,1)) #test size=0.2\n",
        "    #x_va = np.reshape(x_va,(3097,1,100, 5,1))#test size=0.2\n",
        "    X_test= np.reshape(X_test,(3871,1,100, 5,1))# 5d dimension\n",
        "    #print(\"val\",x_va.shape)\n",
        "    #print(x_tr.shape)\n",
        "    list_of_numbers = get_auc()\n",
        "    avg = sum(list_of_numbers)/len(list_of_numbers)\n",
        "    print(\"The average is \", round(avg,5))\n",
        "    i+1"
      ],
      "metadata": {
        "id": "K9GnGrDY_-YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U5ORH6NrAOvT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}